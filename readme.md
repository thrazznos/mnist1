# TinyGrad MNIST Classifier

## Overview

This project demonstrates how to use tinygrad to build and train a simple neural network for recognizing handwritten digits from the MNIST dataset. Tinygrad is a lightweight deep learning framework that aims to be minimalistic and educational, making it an excellent choice for learning about neural networks.

### Features

- Loads and preprocesses the MNIST dataset

- Defines a simple feedforward neural network using tinygrad

- Trains the model using gradient descent

- Evaluates the model's performance on test data

- Predicts digits from the MNIST dataset

### Installation

To run this project, you need Python 3.x and the following dependencies:

Jupyter notebooks

### Usage

Run the jupyter cells in descending order


### Model Architecture

The neural network consists of:

An input layer of 784 neurons (28x28 pixels flattened)

One or more hidden layers with ReLU activation

An output layer of 10 neurons (digits 0-9) with softmax activation

### Acknowledgments

TinyGrad by George Hotz

MNIST Dataset

License

This project is open-source under the MIT License.